apiVersion: v1
kind: ConfigMap
metadata:
  name: semantic-router-config
  namespace: vllm-semantic-router-system
  labels:
    app: semantic-router
    component: configuration
data:
  config.yaml: |
    # ============================================================================
    # Semantic Router Configuration
    # ============================================================================
    # This configuration file defines how the semantic router classifies requests,
    # applies security policies, and routes to backend LLM services.
    
    # ============================================================================
    # BERT Model Configuration - Semantic Embedding
    # ============================================================================
    # Used for semantic similarity matching and caching
    bert_model:
      model_id: sentence-transformers/all-MiniLM-L12-v2
      threshold: 0.6
      use_cpu: true  # Use CPU inference for BERT embeddings
    
    # ============================================================================
    # Semantic Cache Configuration
    # ============================================================================
    # Caches responses based on semantic similarity to reduce redundant inference
    semantic_cache:
      enabled: true
      backend_type: "memory"  # Options: "memory" or "milvus"
      similarity_threshold: 0.8  # How similar queries must be for cache hit
      max_entries: 1000  # Maximum cached responses (memory backend only)
      ttl_seconds: 3600  # Time-to-live: 1 hour
      eviction_policy: "fifo"  # First-in-first-out when max_entries reached
    
    # ============================================================================
    # Tools Database Configuration
    # ============================================================================
    # Automatic tool selection based on query content
    tools:
      enabled: true
      top_k: 3  # Return top 3 most relevant tools
      similarity_threshold: 0.2
      tools_db_path: "config/tools_db.json"
      fallback_to_empty: true  # Return empty list if no tools match
    
    # ============================================================================
    # Prompt Guard - Jailbreak Detection
    # ============================================================================
    # Detects and blocks jailbreak attempts and prompt injections
    prompt_guard:
      enabled: true
      use_modernbert: true
      model_id: "models/jailbreak_classifier_modernbert-base_model"
      threshold: 0.7  # Confidence threshold for jailbreak detection
      use_cpu: true
      jailbreak_mapping_path: "models/jailbreak_classifier_modernbert-base_model/jailbreak_type_mapping.json"
    
    # ============================================================================
    # vLLM Endpoint Configuration
    # ============================================================================
    # IMPORTANT: These endpoints currently point to non-existent services
    # When you deploy actual LLM services, update these addresses to:
    # - Kubernetes service names (e.g., "model-a-service.namespace.svc.cluster.local")
    # - Or external endpoints if using hosted models
    vllm_endpoints:
      - name: "mistral-small-24b-instruct-2501-fp8-dynamic-150"
        address: 172.30.95.11  # this is the service IP in the cluster
        port: 80
        weight: 1  # Load balancing weight

      - name: "qwen-14b-quant"
        address: 172.30.68.65  # this is the service IP in the cluster
        port: 80
        weight: 1
    
    # ============================================================================
    # Model Configuration - Per-Model Settings
    # ============================================================================
    # Defines model-specific behavior, reasoning capabilities, and PII policies
    model_config:
      "mistral-small-24b-instruct-2501-fp8-dynamic-150":
        preferred_endpoints: ["mistral-small-24b-instruct-2501-fp8-dynamic-150"]
        pii_policy:
          allow_by_default: true  # Block all PII by default
          pii_types_allowed: ["EMAIL_ADDRESS"]  # Only allow emails through
      
      "qwen-14b-quant":
        reasoning_family: "qwen3"
        preferred_endpoints: ["qwen-14b-quant"]
        pii_policy:
          allow_by_default: true
          pii_types_allowed: ["EMAIL_ADDRESS"]
    
    # ============================================================================
    # Classifier Configuration - ModernBERT Models
    # ============================================================================
    # Configuration for the classification models baked into your container image
    classifier:
      # Category classification - determines query topic/domain
      category_model:
        model_id: "models/category_classifier_modernbert-base_model"
        use_modernbert: true
        threshold: 0.6  # Minimum confidence for category classification
        use_cpu: true  # Use CPU inference (Rust/Candle)
        category_mapping_path: "models/category_classifier_modernbert-base_model/category_mapping.json"
      
      # PII detection - identifies personally identifiable information
      pii_model:
        model_id: "models/pii_classifier_modernbert-base_presidio_token_model"
        use_modernbert: true
        threshold: 0.7
        use_cpu: true
        pii_mapping_path: "models/pii_classifier_modernbert-base_presidio_token_model/pii_type_mapping.json"
    
    # ============================================================================
    # Category Definitions - 15 Specialized Categories
    # ============================================================================
    # https://vllm-semantic-router.com/docs/overview/categories/supported-categories    
    # Each category has:
    # - name: Category identifier
    # - system_prompt: Contextual prompt for the LLM
    # - model_scores: Which model to use and whether to enable reasoning
    categories:
      # ------------------------------------------------------------------------
      # STEM Categories
      # ------------------------------------------------------------------------
      
      - name: computer science
        system_prompt: "You are a computer science expert with deep knowledge of algorithms, data structures, programming languages, software engineering, databases, networks, and computer architecture. Provide clear, practical solutions with well-commented code examples when helpful. Explain time and space complexity, discuss trade-offs, and follow best practices."
        model_scores:
          - model: qwen-14b-quant
            score: 0.6
            use_reasoning: false
      
      - name: engineering
        system_prompt: "You are an engineering expert with knowledge across mechanical, electrical, civil, chemical, software, and systems engineering disciplines. Apply engineering principles, design methodologies, and problem-solving approaches to provide practical solutions. Consider safety, efficiency, sustainability, and cost-effectiveness. Use technical precision while explaining concepts clearly."
        model_scores:
          - model: qwen-14b-quant
            score: 0.7
            use_reasoning: false
      
      # ------------------------------------------------------------------------
      # Social Sciences and Humanities
      # ------------------------------------------------------------------------
      
      - name: philosophy
        system_prompt: "You are a philosophy expert with comprehensive knowledge of philosophical traditions, ethical theories, logic, metaphysics, epistemology, and political philosophy. Engage with complex philosophical questions by presenting multiple perspectives, analyzing arguments rigorously, and encouraging critical thinking. Draw connections between philosophical concepts and contemporary issues."
        model_scores:
          - model: qwen-14b-quant
            score: 0.5
            use_reasoning: true
      
      # ------------------------------------------------------------------------
      # Health and Applied Sciences
      # ------------------------------------------------------------------------
      - name: health
        system_prompt: "You are a health and medical information expert with knowledge of anatomy, physiology, diseases, treatments, preventive care, nutrition, and wellness. Provide accurate, evidence-based health information while emphasizing that your responses are for educational purposes only and should never replace professional medical advice, diagnosis, or treatment. Always encourage users to consult healthcare professionals for medical concerns."
        model_scores:
          - model: Model-B
            score: 0.5
            use_reasoning: false
      
      # ------------------------------------------------------------------------
      # General/Fallback Category
      # ------------------------------------------------------------------------
      - name: other
        system_prompt: "You are a helpful, knowledgeable, and versatile assistant. Provide accurate, thoughtful responses across a wide range of topics. When the topic is unclear or spans multiple domains, ask clarifying questions. Be honest about the limits of your knowledge and suggest resources when appropriate."
        model_scores:
          - model: mistral-small-24b-instruct-2501-fp8-dynamic-150
            score: 0.7
            use_reasoning: false
    
    # ============================================================================
    # Default Model - Fallback When Classification Uncertain
    # ============================================================================
    default_model: mistral-small-24b-instruct-2501-fp8-dynamic-150
    
    # ============================================================================
    # Reasoning Family Configurations
    # ============================================================================
    # Defines how different model families enable/disable reasoning mode
    # This is critical for models that support Chain-of-Thought reasoning
    reasoning_families:
      # DeepSeek reasoning models
      deepseek:
        type: "chat_template_kwargs"
        parameter: "thinking"
      
      # Qwen3 reasoning models
      qwen3:
        type: "chat_template_kwargs"
        parameter: "enable_thinking"
      
      # OpenAI-style reasoning (o1, o3)
      gpt-oss:
        type: "reasoning_effort"
        parameter: "reasoning_effort"
      
      gpt:
        type: "reasoning_effort"
        parameter: "reasoning_effort"
    
    # Global default reasoning effort level (low, medium, high)
    default_reasoning_effort: high
    
    # ============================================================================
    # API Configuration - Batch Processing
    # ============================================================================
    api:
      batch_classification:
        max_batch_size: 100  # Maximum requests in a single batch
        concurrency_threshold: 10  # When to start batching
        max_concurrency: 8  # Maximum parallel classifications
        
        # Metrics configuration for performance monitoring
        metrics:
          enabled: true
          detailed_goroutine_tracking: true  # Track Go routine performance
          high_resolution_timing: false  # Fine-grained timing (impacts perf)
          sample_rate: 1.0  # Sample 100% of requests (reduce in production)
          
          # Histogram buckets for latency metrics (in seconds)
          duration_buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 30]
          
          # Histogram buckets for batch size metrics
          size_buckets: [1, 2, 5, 10, 20, 50, 100, 200]
    
    # ============================================================================
    # Observability Configuration - Tracing and Monitoring
    # ============================================================================
    observability:
      tracing:
        enabled: true  # Enable distributed tracing for debugging
        provider: "opentelemetry"  # Options: opentelemetry, openinference, openllmetry
        
        exporter:
          type: "stdout"  # Options: otlp, jaeger, zipkin, stdout
          endpoint: "localhost:4317"  # OTLP endpoint (when type: otlp)
          insecure: true  # Use insecure connection (no TLS) for local dev
        
        sampling:
          type: "always_on"  # Options: always_on, always_off, probabilistic
          rate: 1.0  # Sampling rate for probabilistic (0.0-1.0)
        
        # Resource attributes for identifying this service
        resource:
          service_name: "vllm-semantic-router"
          service_version: "v0.1.0"
          deployment_environment: "development"  # UPDATE: Change to production when ready